{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqQNQBlWlHlbegQGgp8Jql",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bongjoonsiong/Machine-Learning-Models/blob/main/Vector_Based_Movie_Recommendation_System_Using_Qdrant_DB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Introduction**\n",
        "    - Movie recommendation systems have evolved from traditional methods to advanced machine learning and vector databases.\n",
        "    - Focus on using Qdrant DB for efficient storage and recommendation of thousands of video files.\n",
        "\n",
        "2. **Traditional Recommender System**\n",
        "    - Uses machine learning algorithms like SVMs to predict user preferences.\n",
        "    - Three main types:\n",
        "        1. Collaborative Filtering: Collects user preferences to predict interests.\n",
        "        2. Content-Based Filtering: Recommends items based on attributes and past interactions.\n",
        "        3. Hybrid Systems: Combine both approaches to improve effectiveness and address challenges like cold start and data sparsity.\n",
        "\n",
        "3. **Challenges in Traditional Systems**\n",
        "    - Cold start problem and data sparsity.\n",
        "    - Ethical considerations and scalability.\n",
        "    - Integration of contextual information.\n",
        "\n",
        "4. **Entry of Vector Databases**\n",
        "    - Useful for efficient similarity searches in recommendation systems.\n",
        "    - Represent movies as vectors in high-dimensional space to find similar movies.\n",
        "    - Use distance metrics like cosine similarity or Euclidean distance.\n",
        "\n",
        "5. **Scalability of Vector Databases**\n",
        "    - Designed to handle large-scale data with high query performance.\n",
        "    - Essential for large streaming platforms with extensive libraries and user bases.\n",
        "\n",
        "6. **Qdrant Database**\n",
        "    - Uses fast approximate nearest neighbor search, specifically the HNSW algorithm with cosine similarity search.\n",
        "    - Suitable for large-scale movie recommendation systems.\n",
        "\n",
        "7. **Recommender System Architecture with Vector Databases**\n",
        "    - **Candidate Generation**\n",
        "        1. Initial filtering based on language or accent (heuristic filtering).\n",
        "        2. Convert video to textual embeddings using audio-to-text models like Whisper or SpeechRecognition.\n",
        "        3. Store textual embeddings as vectors in Qdrant database.\n",
        "        4. Retrieve similar videos using cosine similarity search in Qdrant.\n",
        "    - **Re-Ranking**\n",
        "        1. Arrange movies based on sentiments in textual information.\n",
        "        2. Use large language models to obtain opinion scores.\n",
        "        3. Re-rank movies for recommendations based on opinion scores."
      ],
      "metadata": {
        "id": "w1R91KD0IEmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Implementation"
      ],
      "metadata": {
        "id": "OBzEkf4JIL6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Important Libraries\n",
        "!pip install -q torch\n",
        "!pip install -q openai moviepy\n",
        "!pip install SpeechRecognition\n",
        "!pip install -q transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q qdrant_client"
      ],
      "metadata": {
        "id": "Ivd9jD5aHR-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import all the necessary Packages:\n",
        "import os\n",
        "import moviepy.editor as mp\n",
        "import os\n",
        "import glob\n",
        "import speech_recognition as sr\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "lc9LgegRHPYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a directory to keep all audio transcriptions.\n",
        "\n",
        "# specify your path\n",
        "path = \"/content/my_directory\"\n",
        "\n",
        "# create directory\n",
        "os.makedirs(path, exist_ok=True)"
      ],
      "metadata": {
        "id": "T3_yy0bIJVfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code to convert the video to textual information"
      ],
      "metadata": {
        "id": "YylaePFqHKy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#directory containing video files\n",
        "source_videos_file_path = r\"/content/drive/MyDrive/qdrant_videos\"\n",
        "\n",
        "#directory for storing audio files\n",
        "destination_audio_files_path = r\"/content/my_directory/audios\"\n",
        "\n",
        "# CSV file for storing transcripts\n",
        "csv_file_path = r\"/content/my_directory/transcripts.csv\"\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(destination_audio_files_path, exist_ok=True)\n",
        "\n",
        "# Initialize recognizer class (for recognizing the speech)\n",
        "r = sr.Recognizer()\n",
        "\n",
        "# Open the CSV file in write mode\n",
        "with open(csv_file_path, 'w', newline='') as csvfile:\n",
        "    # Create a CSV writer\n",
        "    writer = csv.writer(csvfile)\n",
        "    # Write the header row\n",
        "    writer.writerow([\"Video File\", \"Transcript\"])\n",
        "\n",
        "    # Process video frame by frame\n",
        "    for video_file in glob.glob(os.path.join(source_videos_file_path, '*.mp4')):\n",
        "        # Convert video to audio\n",
        "        video_clip = mp.VideoFileClip(video_file)\n",
        "        audio_file_path = os.path.join(destination_audio_files_path, os.path.basename(video_file).replace(\"'\", \"\").replace(\" \", \"_\") + '.wav')\n",
        "        video_clip.audio.write_audiofile(audio_file_path)\n",
        "\n",
        "        # Transcribe audio to text\n",
        "        with sr.AudioFile(audio_file_path) as source:\n",
        "            # read the audio file\n",
        "            audio_text = r.listen(source)\n",
        "            # convert speech to text\n",
        "            try:\n",
        "                transcript = r.recognize_google(audio_text)\n",
        "            except sr.UnknownValueError:\n",
        "                print(\"Google Speech Recognition could not understand audio\")\n",
        "                transcript = \"Error: Could not understand audio\"\n",
        "            except sr.RequestError as e:\n",
        "                print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
        "                transcript = \"Error: Could not request results from Google Speech Recognition service; {0}\".format(e)\n",
        "\n",
        "        # Write the transcript to the CSV file\n",
        "        writer.writerow([video_file, transcript])"
      ],
      "metadata": {
        "id": "Nkrncee4Jsnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcripts in dataframe format"
      ],
      "metadata": {
        "id": "oDtIANeLJ1Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/my_directory/transcripts.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "HNXHnjKhJ4SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###There are some transcripts that “SpeechRecognition” is not able to understand, so we will eliminate the row from the dataframe."
      ],
      "metadata": {
        "id": "IOkL0RW-J-ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[~data['Transcript'].str.startswith('Error')]\n",
        "data.head()"
      ],
      "metadata": {
        "id": "JsNN8hB4KCTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###create a QdrantClient instance with an in-memory database"
      ],
      "metadata": {
        "id": "G_mBW_nQKEtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = QdrantClient(\":memory:\")"
      ],
      "metadata": {
        "id": "1lUSqmCWKKwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###create a collection where our vector embeddings will be stored, with distances measured using cosine similarity search"
      ],
      "metadata": {
        "id": "ESMN5K0lKVbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_collection = \"text_collection\"\n",
        "client.recreate_collection(\n",
        "    collection_name=my_collection,\n",
        "    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE)\n",
        ")"
      ],
      "metadata": {
        "id": "J26-PVUEKW-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use a pre-trained model to extract the embedding layer from dataset. Accomplish this using the transformers library and the GPT-2 model."
      ],
      "metadata": {
        "id": "80VKmnAEzUQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "model = AutoModel.from_pretrained('gpt2')#.to(device) # switch this for GPU"
      ],
      "metadata": {
        "id": "3D_lhk8Azm2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Need to extract movie names and create a new column so that it is known which embeddings belong to which movie."
      ],
      "metadata": {
        "id": "R37XgHp2z399"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_movie_name(file_path):\n",
        "    file_name = file_path.split(\"/\")[-1]  # Get the last part of the path\n",
        "    movie_name = file_name.replace(\".mp4\", \"\").strip()\n",
        "    return movie_name\n",
        "\n",
        "# Apply the function to create the new column\n",
        "data['Movie_Name'] = data['Video File'].apply(extract_movie_name)\n",
        "\n",
        "# Display the DataFrame\n",
        "data[['Video File', 'Movie_Name', 'Transcript']]"
      ],
      "metadata": {
        "id": "Cs71soYE0Sfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a helper function with which it is able to get embeddings for each movie trailer transcript."
      ],
      "metadata": {
        "id": "0Svuu5CA0Wvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(row):\n",
        "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    inputs = tokenizer(row['Transcript'], padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "    # Disable gradient computation for the following operations.\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "    # Return the computed embeddings.\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "AZZ1N-fq0gGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply the embedding function to the dataset. After that, save the embeddings so that don’t have to load them again."
      ],
      "metadata": {
        "id": "Rmp3FZl90utj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['embeddings'] = data.apply(get_embeddings, axis=1)\n",
        "np.save(\"vectors\", np.array(data['embeddings']))"
      ],
      "metadata": {
        "id": "0rXbhQO108E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, create a payload with metadata for each movie transcript."
      ],
      "metadata": {
        "id": "4tqUQGuL1DJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "payload = data[['Transcript', 'Movie_Name', 'embeddings']].to_dict(orient=\"records\")\n"
      ],
      "metadata": {
        "id": "sYOWBgQQ1IhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a helper function for mean pooling of token embeddings. Then loop through each transcript in the transcript column to create text embeddings."
      ],
      "metadata": {
        "id": "YiUS-DTj14fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the expected size for the vector embeddings\n",
        "expected_vector_size = 768\n",
        "\n",
        "# Define a function for mean pooling of token embeddings\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    # Extract token embeddings from the model output\n",
        "    token_embeddings = model_output[0]\n",
        "\n",
        "    # Expand the attention mask to match the size of token embeddings\n",
        "    input_mask_expanded = (attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float())\n",
        "\n",
        "    # Calculate the sum of token embeddings, considering the attention mask\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "\n",
        "    # Calculate the sum of the attention mask (clamped to avoid division by zero)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "    # Return the mean-pooled embeddings\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "# Initialize a list to store text embeddings\n",
        "text_embeddings = []\n",
        "\n",
        "# Loop through each transcript in the 'Transcript' column of the 'data' variable\n",
        "for transcript in data['Transcript']:\n",
        "    # Tokenize the transcript, ensuring padding and truncation, and return PyTorch tensors\n",
        "    inputs = tokenizer(transcript, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "    # Perform inference using the model with the tokenized inputs\n",
        "    with torch.no_grad():\n",
        "        embs = model(**inputs)\n",
        "\n",
        "    # Calculate mean-pooled embeddings using the defined function\n",
        "    embedding = mean_pooling(embs, inputs[\"attention_mask\"])\n",
        "\n",
        "    # Ensure the embeddings are of the correct size by trimming or padding\n",
        "    embedding = embedding[:, :expected_vector_size]\n",
        "\n",
        "    # Append the resulting embedding to the list\n",
        "    text_embeddings.append(embedding)"
      ],
      "metadata": {
        "id": "TPs2ycrr2Afm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Assign each transcript an explicit ID within the Qdrant database collection, then create a list of IDs and then upsert the combination of IDs, vectors, and payloads."
      ],
      "metadata": {
        "id": "t72gBu6u2JxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = list(range(len(data)))\n",
        "\n",
        "# Convert PyTorch tensors to lists of floats\n",
        "text_embeddings_list = [[float(num) for num in emb.numpy().flatten().tolist()[:expected_vector_size]] for emb in text_embeddings]\n",
        "\n",
        "client.upsert(collection_name=my_collection,\n",
        "              points=models.Batch(\n",
        "                  ids=ids,\n",
        "                  vectors=text_embeddings_list,\n",
        "                  payloads=payload\n",
        "                  )\n",
        "              )"
      ],
      "metadata": {
        "id": "TQZjypY72TRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a **sentiment analysis model**, you can generate a sentiment score where sentiment polarity between -1 and 1 will be calculated. A score of -1 will indicate negative sentiments, 0 will indicate neutral sentiment, and 1 will indicate positive sentiment."
      ],
      "metadata": {
        "id": "ESw7grAmzcFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def calculate_sentiment_score(text):\n",
        "    # Create a TextBlob object\n",
        "    blob = TextBlob(text)\n",
        "\n",
        "    # Get the sentiment polarity (-1 to 1, where -1 is negative, 0 is neutral, and 1 is positive)\n",
        "    sentiment_score = blob.sentiment.polarity\n",
        "\n",
        "    return sentiment_score\n",
        "\n",
        "# Example usage:\n",
        "text_example = data['Transcript'].iloc[0]\n",
        "sentiment_score_example = calculate_sentiment_score(text_example)\n",
        "print(f\"Sentiment Score: {sentiment_score_example}\")"
      ],
      "metadata": {
        "id": "sjDotH7szfe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For this example, the resultant sentiment score will be 0.75. Now, we’ll apply the helper function for calculating sentiment score to the ‘data’ dataframe"
      ],
      "metadata": {
        "id": "uxSV8weyzu25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Sentiment Score'] = data['Transcript'].apply(calculate_sentiment_score)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "X0DnvwKPzwtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Take the average of the vector embeddings of each movie transcript and combine it with the sentiment score to get the final opinion score."
      ],
      "metadata": {
        "id": "kGgI5TDU08Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['avg_embeddings'] = data['embeddings'].apply(lambda x: np.mean(x, axis=0))\n",
        "data['Opinion_Score'] = 0.7 * data['avg_embeddings'] + 0.3 * data['Sentiment']"
      ],
      "metadata": {
        "id": "s9l0f4ej0-rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In the above code, more weight is assigned to the embeddings because they capture the semantic content and the similarity between movie transcripts. Inherent content similarity is more critical in determining the overall opinion score. The “Sentiment” column defines the emotional tone of the movie transcript. It is assigned with a lower weight because sentiment, as a factor, is not as crucial as semantic content in calculating the overall opinion score. The weights are arbitrary (like we give weightages to train and test sets of a dataset while splitting)."
      ],
      "metadata": {
        "id": "5qSkO-jR_iG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a movie recommender function, which pass a movie name and get the desired number of recommended movies."
      ],
      "metadata": {
        "id": "hjbcwz5PAC_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recommendations(movie_name):\n",
        "    # Find the row corresponding to the given movie name\n",
        "    query_row = data[data['Movie_Name'] == movie_name]\n",
        "\n",
        "    if not query_row.empty:\n",
        "      # Convert the 'Opinion_Score' column to a NumPy array\n",
        "      opinion_scores_array = np.array(data['Opinion_Score'].tolist())\n",
        "      # Upsert the 'Opinion_Score' vectors to the Qdrant collection\n",
        "      opinion_scores_ids = list(range(len(data)))\n",
        "      # Convert the 'Opinion_Score' array to a list of lists\n",
        "      opinion_scores_list = opinion_scores_array.reshape(-1, 1).tolist()\n",
        "\n",
        "      client.upsert(\n",
        "          collection_name=my_collection,\n",
        "          points=models.Batch(\n",
        "              ids=opinion_scores_ids,\n",
        "              vectors=opinion_scores_list\n",
        "              )\n",
        "          )\n",
        "      # Define a query vector based on the opinion score you want to find similar movies for\n",
        "      query_opinion_score = np.array([0.8] * 768)  # Adjust as needed\n",
        "\n",
        "      # Perform a similarity search\n",
        "      search_results = client.search(\n",
        "          collection_name=my_collection,\n",
        "          query_vector=query_opinion_score.tolist(),\n",
        "          limit=3)\n",
        "\n",
        "       # Extract movie recommendations from search results\n",
        "      recommended_movie_ids = [result.id for result in search_results]\n",
        "      recommended_movies = data.loc[data.index.isin(recommended_movie_ids)]\n",
        "\n",
        "      # Display recommended movies\n",
        "      print(\"Recommended Movies:\")\n",
        "      print(recommended_movies[['Movie_Name', 'Opinion_Score']])\n",
        "    else:\n",
        "      print(f\"Movie '{movie_name}' not found in the dataset.\")\n",
        "\n",
        "# Example usage:\n",
        "get_recommendations(\"Star Wars_ The Last Jedi Trailer (Official)\")"
      ],
      "metadata": {
        "id": "YkjsBmv__8zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### This is how Qdrant database is leveraged to create a movie recommender system.\n",
        " Vector databases have many use cases. Among those use cases, movie recommender systems have improved significantly with cosine similarity search and large language models."
      ],
      "metadata": {
        "id": "I9mfoghnAWpF"
      }
    }
  ]
}